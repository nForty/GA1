{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Clustering\n",
    "  \n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "**Part One: KMeans**\n",
    "\n",
    "- Determine the difference between supervised and unsupervised learning.\n",
    "- Demonstrate how to apply k-means clustering.\n",
    "\n",
    "**Part Two: DBScan**\n",
    "\n",
    "- Demonstrate how to apply density-based clustering (DBSCAN).\n",
    "- Define the Silhouette Coefficient and explain how it relates to clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Guide\n",
    "- Unsupervised Learning\n",
    "    - Common Types of Unsupervised Learning\n",
    "    - Unsupervised Learning Example: Coin Clustering\n",
    "- Clustering\n",
    "- Clustering Metrics\n",
    "- K-Means: Centroid Clustering\n",
    "    - K-Means Assumptions\n",
    "    - K-Means Demo\n",
    "    - K-Means Clustering\n",
    "    \n",
    "- Repeat With Scaled Data\n",
    "\n",
    "\n",
    "- DBSCAN: Density-Based Clustering\n",
    "- DBSCAN Clustering Demo\n",
    "- Hierarchical Clustering\n",
    "- Clustering, Classification, and Regression\n",
    "- Comparing Clustering Algorithms\n",
    "- Lesson Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "___\n",
    "> What is the difference between supervised and unsupervised learning?\n",
    "\n",
    "**The primary goal of unsupervised learning is \"representation.\"** Unsupervised learning extracts structure from data. For example, you could segment grocery-store shoppers into \"clusters\" of shoppers who exhibit similar behaviors.\n",
    "\n",
    "**Unsupervised learning is clearly differentiated from supervised learning.** With unsupervised learning:\n",
    "\n",
    "- There's no clear objective.\n",
    "- There's no \"right answer\" (which means it's hard to tell how well you're doing).\n",
    "- There's no response variable — only observations with features.\n",
    "- Labeled data is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Types of Unsupervised Learning\n",
    "\n",
    "**Clustering:** Group “similar” data points together.\n",
    "\n",
    "**Dimensionality Reduction:** Reduce the dimensionality of a data set by extracting features that capture most of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can unsupervised learning be useful?\n",
    "   - Find items with similar behaviour (users, customers, products, voters, etc).\n",
    "   - Market segmentation.\n",
    "   - Understand complex systems.\n",
    "   - Discover meaningful categories for your data.\n",
    "   - Reduce the number of classes by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "   - Reduce the dimensions of your problem.\n",
    "   - Pre-processing! Create labels for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example of Unsupervised Learning: Coin Clustering\n",
    "\n",
    "- Observations: Coins\n",
    "- Features: Size and mass\n",
    "- Response: None (no labels required!)\n",
    "\n",
    "- Perform unsupervised learning:\n",
    "  - Cluster the coins based on “similarity.”  \n",
    "  ![](./assets/unsupervised-coin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering vs classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Classification** - create a model to predict what class an object belongs to.\n",
    " - **Clustering** - find groups that already exist in the data.\n",
    " ![](./assets/classification-vs-clustering.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's focus on finding \"close\" data points and grouping them together. How would you define which points should be in the same group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "\n",
    "We're going to cover three major clustering approaches:\n",
    "\n",
    "- **Centroid clustering using k-means:** Looks for the centers of $k$ pre-defined groups.\n",
    "\n",
    "- **Density-based clustering using DBSCAN:** Looks at gaps, or lack thereof, between data points.\n",
    "\n",
    "- **Hierarchical clustering using agglomerative clustering:** Forms groups of groups of groups in a hierarchy to determine clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "___\n",
    "\n",
    "K-means clustering is a popular centroid-based clustering algorithm.\n",
    " \n",
    "In k-means clustering, we find $k$ clusters (where $k$ is defined by user), each distributed around a single point (called a **centroid**, an imaginary \"center point\" or the \"center of mass\" of a cluster).\n",
    "\n",
    "**K-means seeks to minimize the sum of squares of each point about its cluster centroid.** If we manage to minimize this, then we claim to have found good clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pick a value for $k$ (the number of clusters you want to create).\n",
    "2. Initialise $k$ centroids (starting points) in your data by choosing $k$ random points.\n",
    "3. Assign each data point to a cluster by finding its \"closest\" centroid (e.g. using Euclidean distance).\n",
    "4. Calculate new centroids as means of the data points assigned to each cluster.\n",
    "5. Repeat steps 3-4 until convergence. \n",
    "\n",
    "![](./assets/kmeans.gif)\n",
    "\n",
    "Via [Towards Data Science](https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means algorithm is an **iterative process**; it stops when either:\n",
    "- The centroids and clusters do not change anymore because the clustering has been successful.\n",
    "- The defined maximum number of iterations has been reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) for a demo of k-means clustering in action.\n",
    "\n",
    "![voronoi](assets/voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Assumptions\n",
    "\n",
    "K-means assumes:\n",
    "\n",
    "- k is the correct number of clusters.\n",
    "- The data is isotropically distributed (circular/spherical distribution).\n",
    "- The variance is the same for each variable.\n",
    "- Clusters are roughly the same size.\n",
    "\n",
    "View these resources to see counter examples/cases where assumptions are not met:\n",
    "- [Variance Explained](http://varianceexplained.org/r/kmeans-free-lunch/)\n",
    "- [Scikit-Learn](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy example by hand\n",
    "Let's take a 1-dimensional array of numbers and apply K-Means to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = [2, 5, 6, 8, 12, 15, 18, 28, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick _k=3_ random starting centroids and let the other points fall into clusters based on the closest centroid. Then, reassign your centroids based on the mean value of your cluster and repeat the process.\n",
    "Check with your neighbors. Do you have the same clusters? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we choose _k_?\n",
    "\n",
    "\n",
    "Finding the correct k to use for k-means clustering is not a simple task.\n",
    "\n",
    "We do not have a ground-truth we can use, so there isn't necessarily a **\"correct\"** number of clusters. However, we can find metrics that try to quantify the quality of our groupings.\n",
    "\n",
    "- Sometimes we have good context: We need to create 3 profiles for marketing to target.\n",
    "\n",
    "- Other times we have to figure it out: Our scatter plots show 2 linearly separable clusters.\n",
    "\n",
    "The application is also an important consideration. For example, during customer segmentation we want clusters that are large enough to be targetable by the marketing team. In that case, even if the most natural-looking clusters are small, we may try to group several of them together so that it makes financial sense to target those groups.\n",
    "\n",
    "**Common approaches include:**\n",
    "- Figuring out the correct number of clusters from previous experience.\n",
    "- Using the elbow method to find a number of clusters that no longer seems to improve a clustering metric by a noticeable degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for assessing clustering\n",
    "K-Means is a powerful algorithm, but different starting points may give you different clusters. Besides, the choice of the number of clusters _k_ will significantly affect the resulting clusters. There are two most common approaches to assess the performance of clustering:\n",
    "\n",
    "**Inertia** is a sum of squared errors for each cluster:\n",
    "- ranges from 0 to very high values\n",
    "- **low inertia = dense clusters**\n",
    "\n",
    "$$\\sum_{j=0}^{n} (x_j - \\mu_i)^2$$\n",
    "\n",
    "where $\\mu_i$ is a cluster centroid. K-means explicitly tries to minimize this difference.\n",
    "\n",
    "You can output inertia by calling the `.inertia_` attribute of sklearn's K-Means model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Silhouette Score**](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) is a measure of how far apart clusters are:\n",
    "- ranges from -1 to 1\n",
    "- high silhouette Score = clusters are well separated\n",
    "\n",
    "$$s(i)=\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$$\n",
    "\n",
    "where $a(i)$ is the average distance between the data point $i$ and all other data points in the same cluster, $b(i)$ is the **smallest** average distance of $i$ to all points in any other cluster, of which $i$ is not a member.\n",
    "\n",
    "The definition is a little intricate and can be found [here](https://en.wikipedia.org/wiki/Silhouette_(clustering). **Intuitively the score is based on how much closer data points are to their own clusters than to the nearest neighbour cluster.**\n",
    "\n",
    "We can calculate it in sklearn using `metrics.silhouette_score(X_scaled, labels, metric='euclidean')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-along: applying K-Means to beer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beer data set\n",
    "beer = pd.read_csv('./data/beer.txt', sep=' ')\n",
    "print(beer.shape)\n",
    "beer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How would you cluster these beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X. What about y?\n",
    "X = beer.drop('name', axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means with 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review cluster labels.\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cluster labels and sort by cluster.\n",
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What do the clusters seem to be based on? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the custer centers.\n",
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by clusters and calculate the mean of each feature.\n",
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the `DataFrame` of cluster centroids.\n",
    "centroids = beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a scatter plot of calories versus alcohol, coloured by cluster (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"husl\", 3)\n",
    "\n",
    "sns.scatterplot(data=beer, x='calories', y='alcohol', \n",
    "                s=100, hue='cluster', palette=colors);\n",
    "sns.scatterplot(data=centroids, x='calories', y='alcohol', \n",
    "                linewidth=3, marker='x', \n",
    "                s=200, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a scatter plot matrix, coloured by cluster (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = beer.columns[1:-1]\n",
    "sns.pairplot(beer, x_vars=cols, y_vars= cols, hue='cluster', palette=\"husl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"repeat-with-scaled-data\"></a>\n",
    "### Repeat With Scaled Data\n",
    "\n",
    "Unscaled features cause most algorithms to put too much weight onto one feature. We can scale our data to make sure k-means accounts for all features.\n",
    "\n",
    "Remember that k-means is looking for isotropic groups, meaning that they disperse from the center in all directions evenly. \n",
    "\n",
    "There is more than one choice of scaling method (min/max, z-score, log, etc.), but the best choice is the one that makes your clusters isotropic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Center and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means with 3 clusters on scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cluster labels and sort by cluster.\n",
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the custer centers.\n",
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Are the results of clustering different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Create a scatter plot matrix of new cluster assignments (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(beer, x_vars=cols, y_vars= cols, hue='cluster', palette=\"husl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do you notice any cluster assignments that seem a bit odd? How might we explain those?\n",
    "\n",
    "> What would change if we plot X_scaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "beer_scaled['cluster'] = beer.cluster\n",
    "beer_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(beer_scaled, x_vars=cols, y_vars= cols, hue='cluster', palette=\"husl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods  = pd.read_csv('./data/nutrients.txt', sep=r'\\s+')\n",
    "foods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create K-Means with 2 clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the cluster labels and centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot matrix of cluster assignments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate inertia and silhouette score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN: Density-Based Clustering\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), clusters are created from areas of high density. This can lead to irregularly shaped regions. Also, many parts of space may not belong to any region.\n",
    "![](./assets/dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN: Density-Based Spatial Clustering of Applications With Noise (1996)**\n",
    "\n",
    "The main idea of DBSCAN is to group together closely packed points by identifying:\n",
    "- Core points\n",
    "- Reachable points\n",
    "- Outliers (not reachable)\n",
    "\n",
    "**Its two parameters are:**\n",
    "- `min_samples`: At least this many points are required inside a neighborhood to form a dense cluster.\n",
    "- `eps`: epsion. This is the radius of a neighborhood.\n",
    "\n",
    "**How does it work?** \n",
    "\n",
    "1. Choose a random unvisited data point.\n",
    "2. Find all points in its neighborhood (i.e. at most `eps` units away). Then:\n",
    "    - **If there are at least `min_samples` points in its neighborhood:** Add all points in the neighborhood to the current cluster. Mark them as unvisited if they have not been visited.\n",
    "    - **Otherwise:** Mark the current point as visited. If the point is not part of a cluster, label the point as noise and go to Step 1.\n",
    "3. If another point in the current cluster is unvisited, choose another point in the cluster and go to Step 2. Otherwise, start a new cluster and go to Step 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) for a demo of DBSCAN in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN advantages**:\n",
    "- Can find arbitrarily shaped clusters.\n",
    "- Don’t have to specify number of clusters.\n",
    "- Excludes outliers automatically.\n",
    "\n",
    "**DBSCAN disadvantages**:\n",
    "- Doesn’t work well when clusters are of varying densities.\n",
    "- Hard to choose parameters that work for all clusters.\n",
    "- Can be hard to choose correct parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-Along: Applying DBSCAN to beer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Center and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beer data set\n",
    "beer = pd.read_csv('./data/beer.txt', sep=' ')\n",
    "print(beer.shape)\n",
    "beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Center and scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(beer.drop('name', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5, min_samples=3)\n",
    "db.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the cluster labels.\n",
    "db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = db.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the cluster centroids\n",
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = beer.columns[1:-1]\n",
    "sns.pairplot(beer, x_vars=cols, y_vars= cols, hue='cluster', palette=\"husl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In hierarchical clustering, clusters are composed by joining two smaller clusters together. The algorithm seeks to do exactly what the name suggests:\n",
    "\n",
    "- Build hierarchies of clusters.\n",
    "- Connect the clusters in the hierarchy with links.\n",
    "\n",
    "Once the links are determined, we can display them in what is called a **dendrogram** — a graph that displays all of these links in their hierarchical structure.\n",
    "\n",
    "Below, we see a tree data structure that stores clusters of points:\n",
    "- Each node represents a cluster of one or more data points.\n",
    "- Each leaf represents a single data point.\n",
    "- The root is the cluster containing all data points.\n",
    "- Each parent combines its children's clusters to create a new (larger) cluster.\n",
    "\n",
    "![](./assets/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A simple algorithm we could use to generate this tree:**\n",
    "\n",
    "1. Create a cluster for each point, containing only that point. (Create all leaf nodes.)\n",
    "2. Choose the two clusters with centroids closest to each other.\n",
    "    - Combine the two clusters into a new cluster that replaces the two individual clusters. (Create a new parent node.)\n",
    "3. Repeat Step 2 until only one cluster remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "agg.fit(X_scaled)\n",
    "labels = agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cluster labels and sort by cluster.\n",
    "beer['cluster'] = agg.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "beer_scaled['cluster'] = beer.cluster\n",
    "beer_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = beer_scaled.columns[0:-1]\n",
    "\n",
    "my_palette = dict(zip(beer_scaled.cluster.unique(), sns.color_palette(\"husl\", 3)))\n",
    "row_colors = beer_scaled.cluster.map(my_palette)\n",
    "\n",
    "sns.clustermap(beer_scaled[cols], \n",
    "               metric='euclidean', method='ward', \n",
    "               col_cluster=False,\n",
    "               cmap=sns.color_palette('RdBu_r', 100), robust=True, \n",
    "               row_colors=row_colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering, Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use clustering to discover new features, then use those features for either classification or regression.\n",
    "\n",
    "For classification, we could use clusters directly to classify new points.\n",
    "\n",
    "For regression, we could use a dummy variable for the clusters as a variable in our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function to assign the colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_colors(labels, colors='rgbykcm'):\n",
    "    colored_labels = []\n",
    "    for label in labels:\n",
    "        colored_labels.append(colors[label])\n",
    "    return colored_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "data = []\n",
    "dist = multivariate_normal(mean=[0, 0], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    p = list(dist.rvs())\n",
    "    data.append(dist.rvs())\n",
    "    \n",
    "dist = multivariate_normal(mean=[1, 5], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "    \n",
    "dist = multivariate_normal(mean=[2, 10], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
    "df.head()\n",
    "plt.scatter(df['x'], df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a DBSCAN estimator.\n",
    "estimator = DBSCAN(eps=0.8, min_samples=10)\n",
    "X = df[[\"x\", \"y\"]]\n",
    "estimator.fit(X)\n",
    "# Clusters are given in the labels_ attribute.\n",
    "labels = estimator.labels_\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add cluster labels back to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that -1 clusters are outliers.\n",
    "df[\"cluster\"] = labels\n",
    "df = pd.concat([df, pd.get_dummies(df['cluster'], prefix=\"cluster\")], axis=1)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit a linear model with clusters included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\", \"cluster_0\", \"cluster_1\", \"cluster_2\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "\n",
    "print((model.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What happens if we don't include the clusters we estimated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "print((model.score(X, y)))\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-clustering-algorithms\"></a>\n",
    "## Comparing Clustering Algorithms\n",
    "\n",
    "- K-means\n",
    "  - Finds cluster centers.\n",
    "  - Must choose the number of clusters.\n",
    "  - Assumes clusters are isotropic.\n",
    "- DBSCAN\n",
    "  - Inspects local density to find clusters.\n",
    "  - Better than k-means for anisotropic clusters.\n",
    "  - Capable of finding outliers.\n",
    "- Hierarchical clustering\n",
    "  - Finds clusters by forming groups of groups of groups of points.\n",
    "  - Hierarchical clustering works well for non-spherical clusters.\n",
    "  - May be computationally expensive.\n",
    "  - Guaranteed to converge to the same solution (no random initialization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
